<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="libtorch,">










<meta name="description" content="前面的章节中我们介绍了libtorch的环境搭建（VS和Qt），libtorch张量常用操作，简单的MLP，CNN和LSTM模型搭建，以及数据加载类的使用。本章将以图像分类任务为例，详细介绍如何使用c++训练一个图片分类器。 模型本文以VGG为例，对比pytorch下的模型搭建和训练，阐述Libtorch的模型搭建，模型加载预训练（from ImageNet）权重。VGG模型是2014年的Imag">
<meta name="keywords" content="libtorch">
<meta property="og:type" content="article">
<meta property="og:title" content="libtorch教程（五）">
<meta property="og:url" content="https://allentdan.github.io/2021/01/18/libtorch教程（五）/index.html">
<meta property="og:site_name" content="Allent&#39;s Blogs">
<meta property="og:description" content="前面的章节中我们介绍了libtorch的环境搭建（VS和Qt），libtorch张量常用操作，简单的MLP，CNN和LSTM模型搭建，以及数据加载类的使用。本章将以图像分类任务为例，详细介绍如何使用c++训练一个图片分类器。 模型本文以VGG为例，对比pytorch下的模型搭建和训练，阐述Libtorch的模型搭建，模型加载预训练（from ImageNet）权重。VGG模型是2014年的Imag">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/AllentDan/ImageBase/main/libtorch_deploy/vgg_training.PNG">
<meta property="og:updated_time" content="2021-02-03T11:32:59.216Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="libtorch教程（五）">
<meta name="twitter:description" content="前面的章节中我们介绍了libtorch的环境搭建（VS和Qt），libtorch张量常用操作，简单的MLP，CNN和LSTM模型搭建，以及数据加载类的使用。本章将以图像分类任务为例，详细介绍如何使用c++训练一个图片分类器。 模型本文以VGG为例，对比pytorch下的模型搭建和训练，阐述Libtorch的模型搭建，模型加载预训练（from ImageNet）权重。VGG模型是2014年的Imag">
<meta name="twitter:image" content="https://raw.githubusercontent.com/AllentDan/ImageBase/main/libtorch_deploy/vgg_training.PNG">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://allentdan.github.io/2021/01/18/libtorch教程（五）/">





  <title>libtorch教程（五） | Allent's Blogs</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Allent's Blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://allentdan.github.io/2021/01/18/libtorch教程（五）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Allent Dan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiegeng.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Allent's Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">libtorch教程（五）</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-18T19:50:26+08:00">
                2021-01-18
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/01/18/libtorch教程（五）/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2021/01/18/libtorch教程（五）/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2021/01/18/libtorch教程（五）/" class="leancloud_visitors" data-flag-title="libtorch教程（五）">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">本文总阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>前面的章节中我们介绍了libtorch的环境搭建（VS和Qt），libtorch张量常用操作，简单的MLP，CNN和LSTM模型搭建，以及数据加载类的使用。本章将以图像分类任务为例，详细介绍如何使用c++训练一个图片分类器。</p>
<h1><span id="模型">模型</span></h1><p>本文以VGG为例，对比pytorch下的模型搭建和训练，阐述Libtorch的模型搭建，模型加载预训练（from ImageNet）权重。VGG模型是2014年的ImageNet分类冠军，由于后续深度学习的发展，添加了一些成分，如BatchNorm，形成一些新变种。本文以vgg16bn为例作介绍，vgg16bn就是vgg16加上了后续提出的BatchNorm层。</p>
<h2><span id="分析模型">分析模型</span></h2><p>首先介绍pytorch的模型源码，pytorch的torchvision.models.VGG中有提供官方的VGG模型代码。直接复制上来分析：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, num_classes=<span class="number">1000</span>, init_weights=True)</span>:</span></span><br><span class="line">        super(VGG, self).__init__()</span><br><span class="line">        self.features = features</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_layers</span><span class="params">(cfg, batch_norm=False)</span>:</span></span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">'M'</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv2d, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">'A'</span>: [<span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'B'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'D'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'E'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_vgg</span><span class="params">(arch, cfg, batch_norm, pretrained, progress, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        kwargs[<span class="string">'init_weights'</span>] = <span class="literal">False</span></span><br><span class="line">    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[arch],</span><br><span class="line">                                              progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>和现在的复杂模型相比，VGG模型结构较为简单，就是简单的多次卷积+下采样堆叠，后接一个三层的MLP。代码中VGG模型类有三个成员函数，一个初始化函数<strong>init</strong>，一个前向传播函数forward，最后一个权重初始化函数。类外部有一个函数make_layers函数用于生成CNN主干，返回一个nn.Sequential对象。</p>
<p>打开python编辑器（or IDE，默认有pytorch编程经验）。输入下面代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16,vgg16_bn</span><br><span class="line"></span><br><span class="line">model = vgg16_bn(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(k)</span><br></pre></td></tr></table></figure></p>
<p>发现打印出模型每一层（有权重的层，不包括类似激活函数层）的名称。打印的层名称如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">features.0.weight</span><br><span class="line">features.0.bias</span><br><span class="line">features.1.weight</span><br><span class="line">features.1.bias</span><br><span class="line">features.3.weight</span><br><span class="line">features.3.bias</span><br><span class="line">features.4.weight</span><br><span class="line">features.4.bias</span><br><span class="line">features.7.weight</span><br><span class="line">features.7.bias</span><br><span class="line">features.8.weight</span><br><span class="line">features.8.bias</span><br><span class="line">features.10.weight</span><br><span class="line">features.10.bias</span><br><span class="line">features.11.weight</span><br><span class="line">features.11.bias</span><br><span class="line">features.14.weight</span><br><span class="line">features.14.bias</span><br><span class="line">features.15.weight</span><br><span class="line">features.15.bias</span><br><span class="line">features.17.weight</span><br><span class="line">features.17.bias</span><br><span class="line">features.18.weight</span><br><span class="line">features.18.bias</span><br><span class="line">features.20.weight</span><br><span class="line">features.20.bias</span><br><span class="line">features.21.weight</span><br><span class="line">features.21.bias</span><br><span class="line">features.24.weight</span><br><span class="line">features.24.bias</span><br><span class="line">features.25.weight</span><br><span class="line">features.25.bias</span><br><span class="line">features.27.weight</span><br><span class="line">features.27.bias</span><br><span class="line">features.28.weight</span><br><span class="line">features.28.bias</span><br><span class="line">features.30.weight</span><br><span class="line">features.30.bias</span><br><span class="line">features.31.weight</span><br><span class="line">features.31.bias</span><br><span class="line">features.34.weight</span><br><span class="line">features.34.bias</span><br><span class="line">features.35.weight</span><br><span class="line">features.35.bias</span><br><span class="line">features.37.weight</span><br><span class="line">features.37.bias</span><br><span class="line">features.38.weight</span><br><span class="line">features.38.bias</span><br><span class="line">features.40.weight</span><br><span class="line">features.40.bias</span><br><span class="line">features.41.weight</span><br><span class="line">features.41.bias</span><br><span class="line">classifier.0.weight</span><br><span class="line">classifier.0.bias</span><br><span class="line">classifier.3.weight</span><br><span class="line">classifier.3.bias</span><br><span class="line">classifier.6.weight</span><br><span class="line">classifier.6.bias</span><br></pre></td></tr></table></figure></p>
<p>还行，不是很长。这步操作对后续模型搭建和加载权重很重要，因为torch的模型加载必须要有一一对应的权重层的名称。如果代码中的模型和加载路径对应的权重提供的权重层名称不一致，就会产生错误。</p>
<p>分析模型打印的名称，其实就会发现只有features，classifier，weight和bias和数字。联系前面的官方代码的初始化函数<strong>init</strong>，函数内部有self.classifer和self.features，就很容易得出pytorch模型的内部层名称命名规律了。weight和bias对应conv层里的self.conv和self.bias。点和数字表示nn.Sequential里的序号。</p>
<h2><span id="搭建模型">搭建模型</span></h2><p>下面在c++中搭建一个和pytorch下完全一致的vgg16bn。如果不一致的话其实不影响正常的模型训练和预测，但是影响初始化状态，模型加载从ImageNet数据集训练好的权重以后，训练收敛的速度和收敛后的精度都会好很多。</p>
<p>首先是.h文件中要做的，一个conv_options确定卷积超参数，因为常用所以inline一下。maxpool_options函数确定MaxPool2d的超参数。如何定义一个和pytorch一致的make_features函数，再在VGG类中声明和pytorch一致的初始化和前向传播函数。最后则是一个vgg16bn函数，返回vgg16bn模型。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//和前面章节一致，定义一个确定conv超参数的函数</span></span><br><span class="line"><span class="keyword">inline</span> torch::nn::<span class="function">Conv2dOptions <span class="title">conv_options</span><span class="params">(<span class="keyword">int64_t</span> in_planes, <span class="keyword">int64_t</span> out_planes, <span class="keyword">int64_t</span> kerner_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int64_t</span> stride = <span class="number">1</span>, <span class="keyword">int64_t</span> padding = <span class="number">0</span>, <span class="keyword">bool</span> with_bias = <span class="literal">false</span>)</span> </span>&#123;</span><br><span class="line">    torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size);</span><br><span class="line">    conv_options.stride(stride);</span><br><span class="line">    conv_options.padding(padding);</span><br><span class="line">    conv_options.bias(with_bias);</span><br><span class="line">    <span class="keyword">return</span> conv_options;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//仿照上面的conv_options，定义一个确定MaxPool2d的超参数的函数</span></span><br><span class="line"><span class="keyword">inline</span> torch::nn::<span class="function">MaxPool2dOptions <span class="title">maxpool_options</span><span class="params">(<span class="keyword">int</span> kernel_size, <span class="keyword">int</span> stride)</span></span>&#123;</span><br><span class="line">    torch::nn::<span class="function">MaxPool2dOptions <span class="title">maxpool_options</span><span class="params">(kernel_size)</span></span>;</span><br><span class="line">    maxpool_options.stride(stride);</span><br><span class="line">    <span class="keyword">return</span> maxpool_options;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//对应pytorch中的make_features函数，返回CNN主体，该主体是一个torch::nn::Sequential对象</span></span><br><span class="line">torch::nn::<span class="function">Sequential <span class="title">make_features</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;cfg, <span class="keyword">bool</span> batch_norm)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//VGG类的声明，包括初始化和前向传播</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGImpl</span>:</span> <span class="keyword">public</span> torch::nn::Module</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    torch::nn::Sequential features_&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    torch::nn::AdaptiveAvgPool2d avgpool&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    torch::nn::Sequential classifier;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    VGGImpl(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;cfg, <span class="keyword">int</span> num_classes = <span class="number">1000</span>, <span class="keyword">bool</span> batch_norm = <span class="literal">false</span>);</span><br><span class="line">    torch::<span class="function">Tensor <span class="title">forward</span><span class="params">(torch::Tensor x)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line">TORCH_MODULE(VGG);</span><br><span class="line"></span><br><span class="line"><span class="comment">//vgg16bn函数的声明</span></span><br><span class="line"><span class="function">VGG <span class="title">vgg16bn</span><span class="params">(<span class="keyword">int</span> num_classes)</span></span>;</span><br></pre></td></tr></table></figure>
<p>然后在.cpp文件中定义好.h文件中的声明。.cpp文件的内容如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">torch::nn::<span class="function">Sequential <span class="title">make_features</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;cfg, <span class="keyword">bool</span> batch_norm)</span></span>&#123;</span><br><span class="line">    torch::nn::Sequential features;</span><br><span class="line">    <span class="keyword">int</span> in_channels = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> v : cfg)&#123;</span><br><span class="line">        <span class="keyword">if</span>(v==<span class="number">-1</span>)&#123;</span><br><span class="line">            features-&gt;push_back(torch::nn::MaxPool2d(maxpool_options(<span class="number">2</span>,<span class="number">2</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">auto</span> conv2d = torch::nn::Conv2d(conv_options(in_channels,v,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>));</span><br><span class="line">            features-&gt;push_back(conv2d);</span><br><span class="line">            <span class="keyword">if</span>(batch_norm)&#123;</span><br><span class="line">                features-&gt;push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(v)));</span><br><span class="line">            &#125;</span><br><span class="line">            features-&gt;push_back(torch::nn::ReLU(torch::nn::ReLUOptions(<span class="literal">true</span>)));</span><br><span class="line">            in_channels = v;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> features;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">VGGImpl::VGGImpl(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;cfg, <span class="keyword">int</span> num_classes, <span class="keyword">bool</span> batch_norm)&#123;</span><br><span class="line">    features_ = make_features(cfg,batch_norm);</span><br><span class="line">    avgpool = torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions(<span class="number">7</span>));</span><br><span class="line">    classifier-&gt;push_back(torch::nn::Linear(torch::nn::LinearOptions(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>)));</span><br><span class="line">    classifier-&gt;push_back(torch::nn::ReLU(torch::nn::ReLUOptions(<span class="literal">true</span>)));</span><br><span class="line">    classifier-&gt;push_back(torch::nn::Dropout());</span><br><span class="line">    classifier-&gt;push_back(torch::nn::Linear(torch::nn::LinearOptions(<span class="number">4096</span>, <span class="number">4096</span>)));</span><br><span class="line">    classifier-&gt;push_back(torch::nn::ReLU(torch::nn::ReLUOptions(<span class="literal">true</span>)));</span><br><span class="line">    classifier-&gt;push_back(torch::nn::Dropout());</span><br><span class="line">    classifier-&gt;push_back(torch::nn::Linear(torch::nn::LinearOptions(<span class="number">4096</span>, num_classes)));</span><br><span class="line"></span><br><span class="line">    features_ = register_module(<span class="string">"features"</span>,features_);</span><br><span class="line">    classifier = register_module(<span class="string">"classifier"</span>,classifier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">torch::Tensor VGGImpl::forward(torch::Tensor x)&#123;</span><br><span class="line">    x = features_-&gt;forward(x);</span><br><span class="line">    x = avgpool(x);</span><br><span class="line">    x = torch::flatten(x,<span class="number">1</span>);</span><br><span class="line">    x = classifier-&gt;forward(x);</span><br><span class="line">    <span class="keyword">return</span> torch::log_softmax(x, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">VGG <span class="title">vgg16bn</span><span class="params">(<span class="keyword">int</span> num_classes)</span></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; cfg_dd = &#123;<span class="number">64</span>, <span class="number">64</span>, <span class="number">-1</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">-1</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>&#125;;</span><br><span class="line">    VGG vgg = VGG(cfg_dd,num_classes,<span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">return</span> vgg;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>由于c++中元组如果太长的话声明也会很长，而列表或者vector只接受同类型的数据，就将原来pytorch中的cfg里的’M’改成-1。在读取cfg时判断由原来的’M’变成判断是否-1即可。</p>
<p>需要注意的是，给模型不同层命名时，代码里只出现了register_module对features和classifier命名，这和pytorch保持一致。</p>
<h2><span id="利用预训练权重">利用预训练权重</span></h2><p>下面查看我们c++定义的模型是否和pytorch完全一致。在主函数中实例化一个VGG的对象，然后打印各个层的名称，代码如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; cfg_16bn = &#123;<span class="number">64</span>, <span class="number">64</span>, <span class="number">-1</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">-1</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>&#125;;</span><br><span class="line"><span class="keyword">auto</span> vgg16bn = VGG(cfg_16bn,<span class="number">1000</span>,<span class="literal">true</span>);</span><br><span class="line"><span class="keyword">auto</span> dict16bn = vgg16bn-&gt;named_parameters();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> n = dict16bn.begin(); n != dict16bn.end(); n++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;(*n).key()&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以发现，各个层名称和pytorch中的模型内部层的名称完全一致。这样我们将pytorch的模型权重保存下来，然后加载到c++中。</p>
<p>保存pytorch模型的权重不能直接用torch.save保存模型，这样存下来的模型不能被c++加载。我们利用部署时常用的torch.jit.script模型来保存。python的保存权重代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16,vgg16_bn</span><br><span class="line"></span><br><span class="line">model=model.to(torch.device(<span class="string">"cpu"</span>))</span><br><span class="line">model.eval()</span><br><span class="line">var=torch.ones((<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line">traced_script_module = torch.jit.trace(model, var)</span><br><span class="line">traced_script_module.save(<span class="string">"vgg16bn.pt"</span>)</span><br></pre></td></tr></table></figure></p>
<p>这样，模型的卷积层，归一化层，线性层的权重就保存到.pt文件中了。下面尝试加载到c++中。c++中的加载代码较为简单，直接在定义好的vgg16bn模型后面加载试试：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; cfg_16bn = &#123;<span class="number">64</span>, <span class="number">64</span>, <span class="number">-1</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">-1</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>&#125;;</span><br><span class="line"><span class="keyword">auto</span> vgg16bn = VGG(cfg_16bn,<span class="number">1000</span>,<span class="literal">true</span>);</span><br><span class="line">torch::load(vgg16bn,<span class="string">"your path to vgg16bn.pt"</span>);</span><br></pre></td></tr></table></figure></p>
<p>如果编译运行到下面的函数中中断，则可能原因有：</p>
<ul>
<li>模型保存错了，无法正确加载</li>
<li>路径不对，没正确指向（可能非常大）</li>
<li>c++中定义的模型和python中定义的不一致，最好打印下来，复制到文件中对比文档。<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Value, <span class="keyword">typename</span>... LoadFromArgs&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">load</span><span class="params">(Value&amp; value, LoadFromArgs&amp;&amp;... args)</span> </span>&#123;</span><br><span class="line">  serialize::InputArchive archive;</span><br><span class="line">  archive.load_from(<span class="built_in">std</span>::forward&lt;LoadFromArgs&gt;(args)...);</span><br><span class="line">  archive &gt;&gt; value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>正常运行过了一般就代表模型已经成功加载了。</p>
<h1><span id="数据加载">数据加载</span></h1><p>和第四章一样，本章还是使用pytorch官网提供的<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="noopener">昆虫分类数据集</a>。下载解压后有train和val文件夹，里面分别有两类昆虫图片。数据加载模块代码和上一章一致，就不重复了，感兴趣的往前面博客翻。</p>
<h1><span id="封装">封装</span></h1><h2><span id="声明">声明</span></h2><p>解决了基本的模型定义和加载，数据加载等问题，下面就可以定义一个Classifier类了。这个类的功能主要有：</p>
<ul>
<li>初始化：在初始化中完成模型挂载，是cpu还是某个gpu；定义好分类器并加载预训练的权重，实现更好更快训练。</li>
<li>训练：可以指定分类器训练的周期数，训练的batch_size，学习率以及模型保存的路径。</li>
<li>预测：传入图片就可以返回分类器预测的类别。</li>
<li>加载权重。</li>
</ul>
<p>类的声明很简单：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    torch::Device device = torch::Device(torch::kCPU);</span><br><span class="line">    VGG vgg = VGG&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Classifier(<span class="keyword">int</span> gpu_id = <span class="number">0</span>);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Initialize</span><span class="params">(<span class="keyword">int</span> num_classes, <span class="built_in">std</span>::<span class="built_in">string</span> pretrained_path)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Train</span><span class="params">(<span class="keyword">int</span> epochs, <span class="keyword">int</span> batch_size, <span class="keyword">float</span> learning_rate, <span class="built_in">std</span>::<span class="built_in">string</span> train_val_dir, <span class="built_in">std</span>::<span class="built_in">string</span> image_type, <span class="built_in">std</span>::<span class="built_in">string</span> save_path)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Predict</span><span class="params">(cv::Mat &amp;image)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">LoadWeight</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> weight)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2><span id="定义">定义</span></h2><p>类的成员函数定义较为复杂：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Classifier::LoadWeight(<span class="built_in">std</span>::<span class="built_in">string</span> weight)&#123;</span><br><span class="line">    torch::load(vgg,weight);</span><br><span class="line">    vgg-&gt;eval();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>LoadWeight没太多要讲的，很简单的加载模型并置为eval()。需要注意的是初始化和训练函数，初始化函数由于模型最后一层的num_class不定，所以不能直接加载之前保存的权重。而训练函数要分别用train和val，并且要注意损失设置等。</p>
<h3><span id="初始化">初始化</span></h3><p>首先是初始化函数，初始化函数首先先定义一个num_class对应的分类器vgg16bn，然后定义一个num_class=1000的vgg16bn。加载时加载后者，然后将权重拷贝至前者中。拷贝过程非常精华，需要读者细细揣摩。除了拷贝参数，初始化还会定义好加载到gpu_id对应的GPU上，或者设置gpu_id小于0加载到cpu上。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">Classifier::Classifier(<span class="keyword">int</span> gpu_id)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (gpu_id &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        device = torch::Device(torch::kCUDA, gpu_id);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        device = torch::Device(torch::kCPU);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Classifier::Initialize(<span class="keyword">int</span> _num_classes, <span class="built_in">std</span>::<span class="built_in">string</span> _pretrained_path)&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; cfg_d = &#123;<span class="number">64</span>, <span class="number">64</span>, <span class="number">-1</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">-1</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">-1</span>&#125;;</span><br><span class="line">    <span class="keyword">auto</span> net_pretrained = VGG(cfg_d,<span class="number">1000</span>,<span class="literal">true</span>);</span><br><span class="line">    vgg = VGG(cfg_d,_num_classes,<span class="literal">true</span>);</span><br><span class="line">    torch::load(net_pretrained, _pretrained_path);</span><br><span class="line">    torch::OrderedDict&lt;<span class="built_in">std</span>::<span class="built_in">string</span>, at::Tensor&gt; pretrained_dict = net_pretrained-&gt;named_parameters();</span><br><span class="line">    torch::OrderedDict&lt;<span class="built_in">std</span>::<span class="built_in">string</span>, at::Tensor&gt; model_dict = vgg-&gt;named_parameters();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> n = pretrained_dict.begin(); n != pretrained_dict.end(); n++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">strstr</span>((*n).key().data(), <span class="string">"classifier"</span>)) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        model_dict[(*n).key()] = (*n).value();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    torch::autograd::GradMode::set_enabled(<span class="literal">false</span>);  <span class="comment">// 使参数可以拷贝</span></span><br><span class="line">    <span class="keyword">auto</span> new_params = model_dict; </span><br><span class="line">    <span class="keyword">auto</span> params = vgg-&gt;named_parameters(<span class="literal">true</span> );</span><br><span class="line">    <span class="keyword">auto</span> buffers = vgg-&gt;named_buffers(<span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; val : new_params) &#123;</span><br><span class="line">        <span class="keyword">auto</span> name = val.key();</span><br><span class="line">        <span class="keyword">auto</span>* t = params.find(name);</span><br><span class="line">        <span class="keyword">if</span> (t != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            t-&gt;copy_(val.value());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            t = buffers.find(name);</span><br><span class="line">            <span class="keyword">if</span> (t != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                t-&gt;copy_(val.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    torch::autograd::GradMode::set_enabled(<span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        vgg-&gt;to(device);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (<span class="keyword">const</span> <span class="built_in">std</span>::exception&amp;e)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; e.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3><span id="训练">训练</span></h3><p>然后是训练函数，训练函数分别使用train_loader和val_loader，前者加载train文件夹下的图片训练，后者用于评估。训练过程定义好优化器，损失函数等。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Classifier::Train(<span class="keyword">int</span> num_epochs, <span class="keyword">int</span> batch_size, <span class="keyword">float</span> learning_rate, <span class="built_in">std</span>::<span class="built_in">string</span> train_val_dir, <span class="built_in">std</span>::<span class="built_in">string</span> image_type, <span class="built_in">std</span>::<span class="built_in">string</span> save_path)&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> path_train = train_val_dir+ <span class="string">"\\train"</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> path_val = train_val_dir + <span class="string">"\\val"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> custom_dataset_train = dataSetClc(path_train, image_type).<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line">    <span class="keyword">auto</span> custom_dataset_val = dataSetClc(path_val, image_type).<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> data_loader_train = torch::data::make_data_loader&lt;torch::data::samplers::RandomSampler&gt;(<span class="built_in">std</span>::move(custom_dataset_train), batch_size);</span><br><span class="line">    <span class="keyword">auto</span> data_loader_val = torch::data::make_data_loader&lt;torch::data::samplers::RandomSampler&gt;(<span class="built_in">std</span>::move(custom_dataset_val), batch_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> loss_train = <span class="number">0</span>; <span class="keyword">float</span> loss_val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> acc_train = <span class="number">0.0</span>; <span class="keyword">float</span> acc_val = <span class="number">0.0</span>; <span class="keyword">float</span> best_acc = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> epoch = <span class="number">1</span>; epoch &lt;= num_epochs; ++epoch) &#123;</span><br><span class="line">        <span class="keyword">size_t</span> batch_index_train = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">size_t</span> batch_index_val = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (epoch == <span class="keyword">int</span>(num_epochs / <span class="number">2</span>)) &#123; learning_rate /= <span class="number">10</span>; &#125;</span><br><span class="line">        torch::optim::<span class="function">Adam <span class="title">optimizer</span><span class="params">(vgg-&gt;parameters(), learning_rate)</span></span>; <span class="comment">// 学习率</span></span><br><span class="line">        <span class="keyword">if</span> (epoch &lt; <span class="keyword">int</span>(num_epochs / <span class="number">8</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> mm : vgg-&gt;named_parameters())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">strstr</span>(mm.key().data(), <span class="string">"classifier"</span>))</span><br><span class="line">                &#123;</span><br><span class="line">                    mm.value().set_requires_grad(<span class="literal">true</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    mm.value().set_requires_grad(<span class="literal">false</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> mm : vgg-&gt;named_parameters())</span><br><span class="line">            &#123;</span><br><span class="line">                mm.value().set_requires_grad(<span class="literal">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 遍历data_loader，产生批次</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : *data_loader_train) &#123;</span><br><span class="line">            <span class="keyword">auto</span> data = batch.data;</span><br><span class="line">            <span class="keyword">auto</span> target = batch.target.squeeze();</span><br><span class="line">            data = data.to(torch::kF32).to(device).div(<span class="number">255.0</span>);</span><br><span class="line">            target = target.to(torch::kInt64).to(device);</span><br><span class="line">            optimizer.zero_grad();</span><br><span class="line">            <span class="comment">// Execute the model</span></span><br><span class="line">            torch::Tensor prediction = vgg-&gt;forward(data);</span><br><span class="line">            <span class="keyword">auto</span> acc = prediction.argmax(<span class="number">1</span>).eq(target).sum();</span><br><span class="line">            acc_train += acc.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;() / batch_size;</span><br><span class="line">            <span class="comment">// 计算损失大小</span></span><br><span class="line">            torch::Tensor loss = torch::nll_loss(prediction, target);</span><br><span class="line">            <span class="comment">// 计算梯度</span></span><br><span class="line">            loss.backward();</span><br><span class="line">            <span class="comment">// 更新权重</span></span><br><span class="line">            optimizer.step();</span><br><span class="line">            loss_train += loss.item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">            batch_index_train++;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Epoch: "</span> &lt;&lt; epoch &lt;&lt; <span class="string">" |Train Loss: "</span> &lt;&lt; loss_train / batch_index_train &lt;&lt; <span class="string">" |Train Acc:"</span> &lt;&lt; acc_train / batch_index_train &lt;&lt; <span class="string">"\r"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//验证</span></span><br><span class="line">        vgg-&gt;eval();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : *data_loader_val) &#123;</span><br><span class="line">            <span class="keyword">auto</span> data = batch.data;</span><br><span class="line">            <span class="keyword">auto</span> target = batch.target.squeeze();</span><br><span class="line">            data = data.to(torch::kF32).to(device).div(<span class="number">255.0</span>);</span><br><span class="line">            target = target.to(torch::kInt64).to(device);</span><br><span class="line">            torch::Tensor prediction = vgg-&gt;forward(data);</span><br><span class="line">            <span class="comment">// 计算损失,NLL和Log_softmax配合形成交叉熵损失</span></span><br><span class="line">            torch::Tensor loss = torch::nll_loss(prediction, target);</span><br><span class="line">            <span class="keyword">auto</span> acc = prediction.argmax(<span class="number">1</span>).eq(target).sum();</span><br><span class="line">            acc_val += acc.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;() / batch_size;</span><br><span class="line">            loss_val += loss.item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">            batch_index_val++;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Epoch: "</span> &lt;&lt; epoch &lt;&lt; <span class="string">" |Val Loss: "</span> &lt;&lt; loss_val / batch_index_val &lt;&lt; <span class="string">" |Valid Acc:"</span> &lt;&lt; acc_val / batch_index_val &lt;&lt; <span class="string">"\r"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (acc_val &gt; best_acc) &#123;</span><br><span class="line">            torch::save(vgg, save_path);</span><br><span class="line">            best_acc = acc_val;</span><br><span class="line">        &#125;</span><br><span class="line">        loss_train = <span class="number">0</span>; loss_val = <span class="number">0</span>; acc_train = <span class="number">0</span>; acc_val = <span class="number">0</span>; batch_index_train = <span class="number">0</span>; batch_index_val = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3><span id="预测">预测</span></h3><p>最后是预测，返回类别ans，中间计算置信度prob。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> Classifier::Predict(cv::Mat&amp; image)&#123;</span><br><span class="line">    cv::resize(image, image, cv::Size(<span class="number">448</span>, <span class="number">448</span>));</span><br><span class="line">    torch::Tensor img_tensor = torch::from_blob(image.data, &#123; image.rows, image.cols, <span class="number">3</span> &#125;, torch::kByte).permute(&#123; <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span> &#125;);</span><br><span class="line">    img_tensor = img_tensor.to(device).unsqueeze(<span class="number">0</span>).to(torch::kF32).div(<span class="number">255.0</span>);</span><br><span class="line">    <span class="keyword">auto</span> prediction = vgg-&gt;forward(img_tensor);</span><br><span class="line">    prediction = torch::softmax(prediction,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> class_id = prediction.argmax(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="keyword">int</span>(class_id.item().toInt());</span><br><span class="line">    <span class="keyword">float</span> prob = prediction[<span class="number">0</span>][ans].item().toFloat();</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>末尾贴一个训练时的图：<br><img src="https://raw.githubusercontent.com/AllentDan/ImageBase/main/libtorch_deploy/vgg_training.PNG" alt>。<br>训练时的参数设置如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> vgg_path = <span class="string">"your path to vgg16_bn.pt"</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> train_val_dir = <span class="string">"your path to hymenoptera_data"</span>;</span><br><span class="line"><span class="function">Classifier <span class="title">classifier</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line">classifier.Initialize(<span class="number">2</span>,vgg_path);</span><br><span class="line">classifier.Train(<span class="number">300</span>,<span class="number">4</span>,<span class="number">0.0003</span>,train_val_dir,<span class="string">".jpg"</span>,<span class="string">"classifer.pt"</span>);</span><br></pre></td></tr></table></figure></p>
<p>其实，周期数设置300时，前面很多个周期都在做固定CNN的迁移学习(or finetune)。可以设置小一些查看直接训练全部模型会怎样，以及思考为何会这样。</p>
<p>至此，libtorch初级教程已经完成，坑很多，作者已经为你踩好，更高级的部分在准备中。</p>
<p>分享不易，如果有用请不吝给我一个👍，转载注明出处：<a href="https://allentdan.github.io/">https://allentdan.github.io/</a><br>代码见<a href="https://github.com/AllentDan/LibtorchTutorials" target="_blank" rel="noopener">LibtorchTutorials</a></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Allent Dan WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Allent Dan Alipay">
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/libtorch/" rel="tag"># libtorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/01/18/libtorch教程（四）/" rel="next" title="libtorch教程（四）">
                <i class="fa fa-chevron-left"></i> libtorch教程（四）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/01/18/c++函数的传参/" rel="prev" title="c++函数的传参">
                c++函数的传参 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiegeng.jpg" alt="Allent Dan">
            
              <p class="site-author-name" itemprop="name">Allent Dan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/AllentDan" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:allentdan@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://space.bilibili.com/65234485" target="_blank" title="Bilibili">
                      
                        <i class="fa fa-fw fa-bilibili"></i>Bilibili</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">分析模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">搭建模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">利用预训练权重</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">数据加载</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">封装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">声明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">预测</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allent Dan</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script>


  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>